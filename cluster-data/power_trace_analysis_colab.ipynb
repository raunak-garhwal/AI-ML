{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Google Data Center Power Trace Analysis\n",
        "\n",
        "This colab demonstrates querying the Google data center power traces with bigquery, visualizing them with [Altair](https://altair-viz.github.io/), and analyzing them in conjunction with the 2019 Google cluster data.\n",
        "\n",
        "**Important:** in order to be able to run the queries you will need to:\n",
        "\n",
        "1. Use the [Cloud Resource Manager](https://console.cloud.google.com/cloud-resource-manager) to Create a Cloud Platform project if you do not already have one.\n",
        "1. [Enable billing](https://support.google.com/cloud/answer/6293499#enable-billing) for the project.\n",
        "1. [Enable BigQuery](https://console.cloud.google.com/flows/enableapi?apiid=bigquery) APIs for the project."
      ],
      "metadata": {
        "id": "n86D0dGs4Lh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin with, we'll authenticate with GCP and import the python libraries necessary to execute this colab."
      ],
      "metadata": {
        "id": "c-ipGY9-arep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Please input your project id\n",
        "import altair as alt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "# Provide credentials to the runtime\n",
        "from google.colab import auth\n",
        "from google.cloud.bigquery import magics\n",
        "\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')\n",
        "project_id = 'google.com:google-cluster-data' #@param {type: \"string\"}\n",
        "# Set the default project id for %bigquery magic\n",
        "magics.context.project = project_id\n",
        "\n",
        "# Use the client to run queries constructed from a more complicated function.\n",
        "client = bigquery.Client(project=project_id)\n"
      ],
      "metadata": {
        "id": "CEhNsC1OPajn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Queries"
      ],
      "metadata": {
        "id": "tLOi9ZDM46oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some examples of using the [bigquery magic](https://cloud.google.com/python/docs/reference/bigquery/latest/index.html) to query the power traces.\n",
        "\n",
        "First we'll calculate the average production utilization for a single power domain.\n"
      ],
      "metadata": {
        "id": "nqX1P6QOOUpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "SELECT\n",
        "  AVG(production_power_util) AS average_production_power_util\n",
        "FROM `google.com:google-cluster-data`.powerdata_2019.cella_pdu10"
      ],
      "metadata": {
        "id": "63jcfAIdWfqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now let's find the minimum and maximum measured power utilization for each cell. We use bigquery [wildcard tables](https://cloud.google.com/bigquery/docs/querying-wildcard-tables) in order to conveniently query all trace tables at once."
      ],
      "metadata": {
        "id": "FxNECyAlVJR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "SELECT\n",
        "  cell,\n",
        "  MIN(measured_power_util) AS minimum_measured_power_util,\n",
        "  MAX(measured_power_util) AS maximum_measured_power_util\n",
        "FROM `google.com:google-cluster-data.powerdata_2019.cell*`\n",
        "GROUP BY cell"
      ],
      "metadata": {
        "id": "YqpYZTniOq-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying the previous query to also group by `pdu` gives us the maximum and minimum measured power utilization per power domain."
      ],
      "metadata": {
        "id": "z4PECIQ0OGYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery\n",
        "SELECT\n",
        "  cell,\n",
        "  pdu,\n",
        "  MIN(measured_power_util) AS minimum_measured_power_util,\n",
        "  MAX(measured_power_util) AS maximum_measured_power_util\n",
        "FROM `google.com:google-cluster-data.powerdata_2019.cell*`\n",
        "GROUP BY cell, pdu\n",
        "ORDER BY maximum_measured_power_util"
      ],
      "metadata": {
        "id": "23wOHXUpNvNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measured and Production Power over Time"
      ],
      "metadata": {
        "id": "NvZLmc5TYB76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide traces for two clusters of Google's new Medium Voltage Power Plane (MVPP) data center design, described in [the paper](https://research.google/pubs/pub49032/). Let's plot the measured and estimated production power utilization of one of these MVPPs: mvpp1. We'll limit this visualization to the first 15 days of the trace period (the first 4320 datapoints of the trace)."
      ],
      "metadata": {
        "id": "i89hzgdGbrrI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery mvpp1_df\n",
        "SELECT\n",
        "  time,\n",
        "  measured_power_util,\n",
        "  production_power_util\n",
        "FROM `google.com:google-cluster-data`.powerdata_2019.celli_mvpp1\n",
        "ORDER BY time\n",
        "LIMIT 4320"
      ],
      "metadata": {
        "id": "WhMI0Y8sYBMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alt.Chart(mvpp1_df).mark_line().transform_fold(\n",
        "    [\"measured_power_util\", \"production_power_util\"]).encode(\n",
        "    x=\"time:Q\",\n",
        "    y=alt.X(\"value:Q\", scale=alt.Scale(zero=False)),\n",
        "    color=\"key:N\"\n",
        ").properties(width=700, height=75)"
      ],
      "metadata": {
        "id": "K07qANn2Zz_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU and Power over Time"
      ],
      "metadata": {
        "id": "bdzTheN-XdHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example of joining the power traces with the cluster traces, we'll plot the average CPU utilization and power utilization per hour.\n",
        "\n",
        "You may remember this query from the [cluster analysis colab](https://github.com/google/cluster-data/blob/master/clusterdata_analysis_colab.ipynb). It's been modified slightly--see `query_per_tier_utilization_time_series` in particular."
      ],
      "metadata": {
        "id": "TL7Dh7-TFsK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def machines_in_pdu(pdu_number):\n",
        "  return '''\n",
        "SELECT machine_id\n",
        "FROM `google.com:google-cluster-data`.powerdata_2019.machine_to_pdu_mapping\n",
        "WHERE pdu = 'pdu{pdu_number}'\n",
        "  '''.format(pdu_number=pdu_number)\n",
        "\n",
        "def query_cell_capacity(cell, pdu_number):\n",
        "  return '''\n",
        "SELECT SUM(cpu_cap) AS cpu_capacity\n",
        "FROM (\n",
        "  SELECT machine_id, MAX(capacity.cpus) AS cpu_cap,\n",
        "  FROM `google.com:google-cluster-data`.clusterdata_2019_{cell}.machine_events\n",
        "  WHERE machine_id IN ({machine_query})\n",
        "  GROUP BY 1\n",
        ")\n",
        "  '''.format(cell=cell, machine_query=machines_in_pdu(pdu_number))\n",
        "\n",
        "def query_per_instance_usage_priority(cell, pdu_num):\n",
        "  return '''\n",
        "SELECT u.time AS time,\n",
        "  u.collection_id AS collection_id,\n",
        "  u.instance_index AS instance_index,\n",
        "  e.priority AS priority,\n",
        "  CASE\n",
        "    WHEN e.priority BETWEEN 0 AND 99 THEN '1_free'\n",
        "    WHEN e.priority BETWEEN 100 AND 115 THEN '2_beb'\n",
        "    WHEN e.priority BETWEEN 116 AND 119 THEN '3_mid'\n",
        "    ELSE '4_prod'\n",
        "  END AS tier,\n",
        "  u.cpu_usage AS cpu_usage\n",
        "FROM (\n",
        "  SELECT start_time AS time,\n",
        "    collection_id,\n",
        "    instance_index,\n",
        "    machine_id,\n",
        "    average_usage.cpus AS cpu_usage\n",
        "  FROM `google.com:google-cluster-data`.clusterdata_2019_{cell}.instance_usage\n",
        "  WHERE (alloc_collection_id IS NULL OR alloc_collection_id = 0)\n",
        "    AND (end_time - start_time) >= (5 * 60 * 1e6)\n",
        ") AS u JOIN (\n",
        "  SELECT collection_id, instance_index, machine_id,\n",
        "    MAX(priority) AS priority\n",
        "  FROM `google.com:google-cluster-data`.clusterdata_2019_{cell}.instance_events\n",
        "  WHERE (alloc_collection_id IS NULL OR alloc_collection_id = 0)\n",
        "    AND machine_id IN ({machine_query})\n",
        "  GROUP BY 1, 2, 3\n",
        ") AS e ON u.collection_id = e.collection_id\n",
        "  AND u.instance_index = e.instance_index\n",
        "  AND u.machine_id = e.machine_id\n",
        "  '''.format(cell=cell, machine_query=machines_in_pdu(pdu_num))\n",
        "\n",
        "def query_per_tier_utilization_time_series(cell, pdu_num, cpu_capacity):\n",
        "  return '''\n",
        "SELECT * FROM (\n",
        "  SELECT CAST(FLOOR(time/(1e6 * 60 * 60)) AS INT64) AS hour_index,\n",
        "    tier,\n",
        "    SUM(cpu_usage) / (12 * {cpu_capacity}) AS avg_cpu_usage,\n",
        "  FROM ({table})\n",
        "  GROUP BY 1, 2)\n",
        "JOIN (\n",
        "  SELECT CAST(FLOOR(time/(1e6 * 60 * 60)) AS INT64) AS hour_index,\n",
        "  pdu,\n",
        "  AVG(measured_power_util) as avg_measured_power_util,\n",
        "  AVG(production_power_util) AS avg_production_power_util\n",
        "  FROM `google.com:google-cluster-data`.`powerdata_2019.cell{cell}_pdu{pdu_num}`\n",
        "  GROUP BY hour_index, pdu\n",
        ") USING (hour_index)\n",
        "  '''.format(table=query_per_instance_usage_priority(cell, pdu_num),\n",
        "             cpu_capacity=cpu_capacity, cell=cell, pdu_num=pdu_num)\n",
        "\n",
        "def run_query_utilization_per_time_time_series(cell, pdu_num):\n",
        "  cell_cap = client.query(query_cell_capacity(cell, pdu_num)).to_dataframe()\n",
        "  query = query_per_tier_utilization_time_series(\n",
        "      cell,\n",
        "      pdu_num,\n",
        "      cell_cap['cpu_capacity'][0])\n",
        "  time_series = client.query(query).to_dataframe()\n",
        "  return time_series\n",
        "\n",
        "CELL='f'\n",
        "PDU_NUM='17'\n",
        "hourly_usage = run_query_utilization_per_time_time_series(CELL, PDU_NUM)"
      ],
      "metadata": {
        "id": "Bmp8Z2diaCPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot power utilization on top of the CPU utilization graph."
      ],
      "metadata": {
        "id": "E47jgIjzGnDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CPU graph\n",
        "cpu = alt.Chart().mark_area().encode(\n",
        "        alt.X('hour_index:N'),\n",
        "        alt.Y('avg_cpu_usage:Q'),\n",
        "        color=alt.Color('tier', legend=alt.Legend(orient=\"left\", title=None)),\n",
        "        order=alt.Order('tier', sort='descending'),\n",
        "        tooltip=['tier:N', 'avg_cpu_usage:Q']\n",
        "  )\n",
        "cpu.encoding.x.title = \"Hour\"\n",
        "cpu.encoding.y.title = \"Average Utilization\"\n",
        "\n",
        "\n",
        "# Power Utilization graph\n",
        "pu = (\n",
        "    alt.Chart()\n",
        "    .transform_fold(['avg_measured_power_util', 'avg_production_power_util'])\n",
        "    .encode(\n",
        "        alt.X(\n",
        "            'hour_index:N',\n",
        "            axis=alt.Axis(labels=False, domain=False, ticks=False),\n",
        "        ),\n",
        "        alt.Y('value:Q'),\n",
        "        color=alt.Color('key:N', legend=None),\n",
        "        strokeDash=alt.StrokeDash('key:N', legend=None),\n",
        "        tooltip=['hour_index:N', 'key:N', 'value:Q']\n",
        "    )\n",
        "    .mark_line().properties(title=alt.datum.cell + ': ' + alt.datum.cluster)\n",
        ")\n",
        "\n",
        "\n",
        "alt.layer(cpu, pu, data=hourly_usage).properties(\n",
        "    width=1200,\n",
        "    height=300,\n",
        "    title=\"Average CPU and Power Utilization\").configure_axis(grid=False)"
      ],
      "metadata": {
        "id": "bUCQxV7-iDPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can adapt the previous queries to calculate the average CPU and power utilizations per day by tier (i.e. cappable workloads or production) for each PDU in cell `b`."
      ],
      "metadata": {
        "id": "G0qxiNg96pq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bigquery cluster_and_power_data_df\n",
        "WITH\n",
        "  machines_in_cell AS (\n",
        "    SELECT machine_id, pdu, cell\n",
        "    FROM `google.com:google-cluster-data`.powerdata_2019.machine_to_pdu_mapping\n",
        "    WHERE cell = 'b'\n",
        "  ),\n",
        "  cpu_capacities AS (\n",
        "    SELECT pdu, cell, SUM(cpu_cap) AS cpu_capacity\n",
        "    FROM\n",
        "      (\n",
        "        SELECT machine_id, MAX(capacity.cpus) AS cpu_cap,\n",
        "        FROM `google.com:google-cluster-data`.clusterdata_2019_b.machine_events\n",
        "        GROUP BY 1\n",
        "      )\n",
        "    JOIN machines_in_cell\n",
        "      USING (machine_id)\n",
        "    GROUP BY 1, 2\n",
        "  ),\n",
        "  per_instance_usage_priority AS (\n",
        "    SELECT\n",
        "      u.time AS time,\n",
        "      u.collection_id AS collection_id,\n",
        "      u.instance_index AS instance_index,\n",
        "      e.priority AS priority,\n",
        "      IF(e.priority < 120, 'cappable', 'production') AS tier,\n",
        "      u.cpu_usage AS cpu_usage,\n",
        "      m.pdu\n",
        "    FROM\n",
        "      (\n",
        "        SELECT\n",
        "          start_time AS time,\n",
        "          collection_id,\n",
        "          instance_index,\n",
        "          machine_id,\n",
        "          average_usage.cpus AS cpu_usage\n",
        "        FROM `google.com:google-cluster-data`.clusterdata_2019_b.instance_usage\n",
        "        WHERE\n",
        "          (alloc_collection_id IS NULL OR alloc_collection_id = 0)\n",
        "          AND (end_time - start_time) >= (5 * 60 * 1e6)\n",
        "      ) AS u\n",
        "    JOIN\n",
        "      (\n",
        "        SELECT collection_id, instance_index, machine_id, MAX(priority) AS priority\n",
        "        FROM `google.com:google-cluster-data`.clusterdata_2019_b.instance_events\n",
        "        WHERE (alloc_collection_id IS NULL OR alloc_collection_id = 0)\n",
        "        GROUP BY 1, 2, 3\n",
        "      ) AS e\n",
        "      ON\n",
        "        u.collection_id = e.collection_id\n",
        "        AND u.instance_index = e.instance_index\n",
        "        AND u.machine_id = e.machine_id\n",
        "    JOIN machines_in_cell AS m\n",
        "      ON m.machine_id = u.machine_id\n",
        "  )\n",
        "SELECT *\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      CAST(FLOOR(time / (1e6 * 60 * 60 * 24)) AS INT64) AS day_index,\n",
        "      pdu,\n",
        "      tier,\n",
        "      SUM(cpu_usage) / (12 * 24 * ANY_VALUE(cpu_capacity)) AS avg_cpu_usage,\n",
        "    FROM per_instance_usage_priority\n",
        "    JOIN cpu_capacities\n",
        "      USING (pdu)\n",
        "    GROUP BY 1, 2, 3\n",
        "  )\n",
        "JOIN\n",
        "  (\n",
        "    SELECT\n",
        "      CAST(FLOOR((time - 6e8 + 3e8) / (1e6 * 60 * 60 * 24)) AS INT64) AS day_index,\n",
        "      pdu,\n",
        "      AVG(measured_power_util) AS avg_measured_power_util,\n",
        "      AVG(production_power_util) AS avg_production_power_util\n",
        "    FROM `google.com:google-cluster-data`.`powerdata_2019.cellb_pdu*`\n",
        "    GROUP BY 1, 2\n",
        "  )\n",
        "  USING (pdu, day_index)"
      ],
      "metadata": {
        "id": "Id_W7pdF6xK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_and_power_data_df.describe()"
      ],
      "metadata": {
        "id": "-DFupvCbKniC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Be careful joining on `time` directly!\n",
        "Note that in the queries above we converted `time` from the cluster and power datasets to an `hour_index` which we then use to join the two datasets. We don't want to join on `time` directly due to the how the datasets are structured.\n",
        "\n",
        "The power trace `time` values are each aligned to the 5-minute mark. For example, there's a `time` at 600s, 900s, 1200s but never 700s or 601s. The cluster trace `time` values have no specific alignment. If we were to join the two datasets on `time`, we'd end up dropping a lot of data!"
      ],
      "metadata": {
        "id": "ok8vgKFvCCgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may have also noticed that there are `day_index` has 32 unique values in the query above, despite May having 31 days. This is because timestamps in the data sets are represented as microseconds since 600 seconsd before the start of the trace period, May 01 2019 at 00:00 PT."
      ],
      "metadata": {
        "id": "ifMNifTPODkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recreating Graphs from the Paper"
      ],
      "metadata": {
        "id": "wbXuUBygOu5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, the power traces are used to re-create figures from [the paper](https://research.google/pubs/pub49032/)."
      ],
      "metadata": {
        "id": "aARdyheiRgVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_data(filter='pdu%', agg_by='pdu', util_type='measured_power_util'):\n",
        "  query = \"\"\"\n",
        "  SELECT bin as bins, SUM(count) as counts\n",
        "  FROM (\n",
        "    SELECT {agg_by},\n",
        "           ROUND(CAST({util_type} / 0.0001 as INT64) * 0.0001, 3) as bin,\n",
        "           COUNT(*) as count\n",
        "    FROM (\n",
        "      SELECT {agg_by},\n",
        "             time,\n",
        "             {util_type},\n",
        "      FROM `google.com:google-cluster-data`.`powerdata_2019.cell*`\n",
        "      WHERE NOT bad_measurement_data\n",
        "            AND NOT bad_production_power_data\n",
        "            AND {agg_by} LIKE '{filter}'\n",
        "            AND NOT cell in ('i', 'j')\n",
        "    )\n",
        "    GROUP BY 1, 2\n",
        "  ) GROUP BY 1 ORDER BY 1;\n",
        "  \"\"\".format(**{'filter': filter, 'agg_by': agg_by, 'util_type': util_type})\n",
        "  return client.query(query).to_dataframe()"
      ],
      "metadata": {
        "id": "LFuWscTAOyE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdu_df = histogram_data()\n",
        "cluster_df = histogram_data('%', 'cell')\n",
        "prod_pdu_df = histogram_data(util_type='production_power_util')\n",
        "prod_cluster_df = histogram_data('%', 'cell', util_type='production_power_util')"
      ],
      "metadata": {
        "id": "62vMXuSYUIG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_cdf(p_df, c_df, title):\n",
        "  pdu_counts, pdu_bins = p_df.counts, p_df.bins\n",
        "  cluster_counts, cluster_bins = c_df.counts, c_df.bins\n",
        "\n",
        "  pdu_cdf = np.cumsum (list(pdu_counts))\n",
        "  pdu_cdf = (1.0 * pdu_cdf) / pdu_cdf[-1]\n",
        "  cluster_cdf = np.cumsum (list(cluster_counts))\n",
        "  cluster_cdf = (1.0 * cluster_cdf) / cluster_cdf[-1]\n",
        "\n",
        "  pdu_cdf_graph = alt.Chart(pd.DataFrame(\n",
        "      {'bins': pdu_bins, 'cdf': pdu_cdf})).mark_point(size=.1).encode(\n",
        "        x=alt.X('bins', scale=alt.Scale(domain=[0.4, 0.90])),\n",
        "        y=alt.Y('cdf'),\n",
        "        color=alt.value('steelblue')\n",
        "  )\n",
        "\n",
        "  cluster_cdf_graph = alt.Chart(pd.DataFrame(\n",
        "      {'bins': cluster_bins, 'cdf': cluster_cdf})).mark_point(size=.1).encode(\n",
        "        x=alt.X('bins', scale=alt.Scale(domain=[0.4, 0.90])),\n",
        "        y=alt.Y('cdf'),\n",
        "        color=alt.value('forestgreen')\n",
        "  )\n",
        "\n",
        "  return (pdu_cdf_graph + cluster_cdf_graph).properties(title=title)\n",
        "\n",
        "\n",
        "alt.hconcat(make_cdf(pdu_df, cluster_df, \"Measured Power Util\"), make_cdf(\n",
        "    prod_pdu_df, prod_cluster_df, \"Production Power Util\"))"
      ],
      "metadata": {
        "id": "eEze9O1CUlSU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}